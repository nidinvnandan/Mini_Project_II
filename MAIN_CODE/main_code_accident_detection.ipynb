{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Project 2 - Road Accident Detection And Alert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted By\n",
    "\n",
    "Course: MSc Computer Science With Data Analytics\n",
    "\n",
    "Name(Reg No:)\n",
    "\n",
    "NIDIN V NANDAN(223039)\n",
    "\n",
    "ADARSH PS(223004)\n",
    "\n",
    "ADHISH S SUJAN(223005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble class for handling multiple YOLOv5 models\n",
    "\n",
    "class Ensemble(nn.ModuleList):\n",
    "    # Ensemble of models\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, augment=False, profile=False, visualize=False):\n",
    "        y = [module(x, augment, profile, visualize)[0] for module in self]\n",
    "       \n",
    "        y = torch.cat(y, 1)  \n",
    "        return y, None  \n",
    "\n",
    "def attempt_load(weights, device=None, inplace=True, fuse=True):\n",
    "    \n",
    "    from models.yolo import Detect, Model\n",
    "\n",
    "    model = Ensemble()\n",
    "    for w in weights if isinstance(weights, list) else [weights]:\n",
    "        ckpt = torch.load('o&a.pt', map_location='cpu') \n",
    "        ckpt = (ckpt.get('ema') or ckpt['model']).to(device).float() \n",
    "\n",
    "       \n",
    "        if not hasattr(ckpt, 'stride'):\n",
    "            ckpt.stride = torch.tensor([32.])\n",
    "        if hasattr(ckpt, 'names') and isinstance(ckpt.names, (list, tuple)):\n",
    "            ckpt.names = dict(enumerate(ckpt.names)) \n",
    "\n",
    "        model.append(ckpt.fuse().eval() if fuse and hasattr(ckpt, 'fuse') else ckpt.eval())  \n",
    "\n",
    "    # Module updates\n",
    "    for m in model.modules():\n",
    "        t = type(m)\n",
    "        if t in (nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU, Detect, Model):\n",
    "            m.inplace = inplace\n",
    "            if t is Detect and not isinstance(m.anchor_grid, list):\n",
    "                delattr(m, 'anchor_grid')\n",
    "                setattr(m, 'anchor_grid', [torch.zeros(1)] * m.nl)\n",
    "        elif t is nn.Upsample and not hasattr(m, 'recompute_scale_factor'):\n",
    "            m.recompute_scale_factor = None\n",
    "    # Return model\n",
    "    if len(model) == 1:\n",
    "        return model[-1]\n",
    "\n",
    "    # Return detection ensemble\n",
    "    print(f'Ensemble created with {weights}\\n')\n",
    "    for k in 'names', 'nc', 'yaml':\n",
    "        setattr(model, k, getattr(model[0], k))\n",
    "    model.stride = model[torch.argmax(torch.tensor([m.stride.max() for m in model])).int()].stride \n",
    "    assert all(model[0].nc == m.nc for m in model), f'Models have different class counts: {[m.nc for m in model]}'\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "    \"\"\"\n",
    "    Rescale bounding boxes from img1_shape to img0_shape.\n",
    "\n",
    "    Args:\n",
    "        img1_shape (tuple): Original image shape (height, width).\n",
    "        boxes (torch.Tensor): Bounding boxes in format (x_center, y_center, width, height).\n",
    "        img0_shape (tuple): Target image shape (height, width).\n",
    "        ratio_pad (tuple): Optional padding ratio for rescaling.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Rescaled bounding boxes.\n",
    "    \"\"\"\n",
    "    if ratio_pad is None: \n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1]) \n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2 \n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    boxes[..., [0, 2]] -= pad[0]  \n",
    "    boxes[..., [1, 3]] -= pad[1]  \n",
    "    boxes[..., :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes\n",
    "\n",
    "def clip_boxes(boxes, shape):\n",
    "    \"\"\"\n",
    "    Clip bounding boxes to fit within image shape.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): Bounding boxes in format (x1, y1, x2, y2).\n",
    "        shape (tuple): Image shape (height, width).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if isinstance(boxes, torch.Tensor):  \n",
    "        boxes[..., 0].clamp_(0, shape[1])\n",
    "        boxes[..., 1].clamp_(0, shape[0]) \n",
    "        boxes[..., 2].clamp_(0, shape[1]) \n",
    "        boxes[..., 3].clamp_(0, shape[0])  \n",
    "    else:  \n",
    "        boxes[..., [0, 2]] = boxes[..., [0, 2]].clip(0, shape[1]) \n",
    "        boxes[..., [1, 3]] = boxes[..., [1, 3]].clip(0, shape[0])\n",
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from [x, y, w, h] to [x1, y1, x2, y2].\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Bounding boxes in format (x, y, w, h).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Converted bounding boxes.\n",
    "    \"\"\"\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  \n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2 \n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2 \n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2 \n",
    "    return y\n",
    "def box_iou(box1, box2, eps=1e-7):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) between two sets of bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        box1 (torch.Tensor): Bounding boxes set 1.\n",
    "        box2 (torch.Tensor): Bounding boxes set 2.\n",
    "        eps (float): Epsilon value to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: IoU between the two sets of bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    (a1, a2), (b1, b2) = box1.unsqueeze(1).chunk(2, 2), box2.unsqueeze(0).chunk(2, 2)\n",
    "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "\n",
    "    # IoU = inter / (area1 + area2 - inter)\n",
    "    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)\n",
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.45,\n",
    "        classes=None,\n",
    "        agnostic=False,\n",
    "        multi_label=False,\n",
    "        labels=(),\n",
    "        max_det=300,\n",
    "        nm=0,  # number of masks\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform Non-Maximum Suppression (NMS) on bounding box predictions.\n",
    "\n",
    "    Args:\n",
    "        prediction (torch.Tensor): Model predictions.\n",
    "        conf_thres (float): Confidence threshold.\n",
    "        iou_thres (float): IoU threshold for NMS.\n",
    "        classes (list): List of classes to consider.\n",
    "        agnostic (bool): If True, NMS will be class-agnostic.\n",
    "        multi_label (bool): If True, allows multiple labels per box.\n",
    "        labels (tuple): A tuple of labels for autolabelling.\n",
    "        max_det (int): Maximum number of detections to keep after NMS.\n",
    "        nm (int): Number of masks.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tensors, each containing selected bounding boxes after NMS.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "    if isinstance(prediction, (list, tuple)):  \n",
    "        prediction = prediction[0]  \n",
    "\n",
    "    device = prediction.device\n",
    "    mps = 'mps' in device.type  \n",
    "    if mps:  \n",
    "        prediction = prediction.cpu()\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - nm - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    \n",
    "    max_wh = 7680  \n",
    "    max_nms = 30000  \n",
    "    time_limit = 0.5 + 0.05 * bs \n",
    "    redundant = True \n",
    "    multi_label &= nc > 1  \n",
    "    merge = False \n",
    "\n",
    "    t = time.time()\n",
    "    mi = 5 + nc \n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "    for xi, x in enumerate(prediction):  \n",
    "        \n",
    "        x = x[xc[xi]]  \n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            lb = labels[xi]\n",
    "            v = torch.zeros((len(lb), nc + nm + 5), device=x.device)\n",
    "            v[:, :4] = lb[:, 1:5]\n",
    "            v[:, 4] = 1.0 \n",
    "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  \n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4]) \n",
    "        mask = x[:, mi:] \n",
    "\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 5 + j, None], j[:, None].float(), mask[i]), 1)\n",
    "        else: \n",
    "            conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float(), mask), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "  \n",
    "        n = x.shape[0] \n",
    "        if not n: \n",
    "            continue\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]] \n",
    "\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh) \n",
    "        boxes, scores = x[:, :4] + c, x[:, 4] \n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres) \n",
    "        i = i[:max_det]  \n",
    "        if merge and (1 < n < 3E3):  \n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  \n",
    "            weights = iou * scores[None]  \n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True) \n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  \n",
    "\n",
    "        output[xi] = x[i]\n",
    "        if mps:\n",
    "            output[xi] = output[xi].to(device)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    \"\"\"\n",
    "    Resize and pad an image while meeting stride-multiple constraints.\n",
    "\n",
    "    Args:\n",
    "        im (numpy.ndarray): Input image.\n",
    "        new_shape (tuple or int): Target shape for the image after resizing.\n",
    "        color (tuple): RGB color value for padding.\n",
    "        auto (bool): If True, compute minimum rectangle for padding.\n",
    "        scaleFill (bool): If True, stretch the image to fit the target shape.\n",
    "        scaleup (bool): If True, allow scaling up for better validation mAP.\n",
    "        stride (int): Stride for constraint.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Resized and padded image.\n",
    "        tuple: Width and height ratios between original and new shapes.\n",
    "        tuple: Width and height padding applied to the image.\n",
    "    \"\"\"\n",
    "    shape = im.shape[:2]  \n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  \n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    ratio = r, r \n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1] \n",
    "    if auto: \n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride) \n",
    "    elif scaleFill:\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0] \n",
    "\n",
    "    dw /= 2 \n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color) \n",
    "    return im, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    \"\"\"\n",
    "    Draw a bounding box with an optional label on an image.\n",
    "\n",
    "    Args:\n",
    "        x (list or tuple): Coordinates of the bounding box in the format [x1, y1, x2, y2].\n",
    "        img (numpy.ndarray): Input image.\n",
    "        color (list): RGB color value for the bounding box and label background.\n",
    "        label (str): Optional label text.\n",
    "        line_thickness (int): Line and font thickness.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tl = line_thickness or round(0.002 * max(img.shape[0:2])) + 1  \n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  \n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA) \n",
    "        cv2.putText(\n",
    "            img,\n",
    "            label,\n",
    "            (c1[0], c1[1] - 2),\n",
    "            0,\n",
    "            tl / 3,\n",
    "            [225, 255, 255],\n",
    "            thickness=tf,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1- Code For Detecting accident Without Sending alert message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = 'o&a.pt'  # Path to model weights\n",
    "model = attempt_load(weights)\n",
    "model.to(device)  \n",
    "stride = int(model.stride.max())  \n",
    "imgsz = 640  # Input image size \n",
    "\n",
    "cap = cv2.VideoCapture(\"accidentvideo.mp4\")  \n",
    "accident_frames_threshold = 3 \n",
    "accident_frames = 0\n",
    "alert_displayed = False\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img0 = letterbox(frame, new_shape=imgsz)[0]\n",
    "    img = img0[:, :, ::-1].transpose(2, 0, 1) \n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float() \n",
    "    img /= 255.0  \n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, 0.4, 0.5, classes=None, agnostic=False)\n",
    "\n",
    "    accident_detected = False\n",
    "\n",
    "    for det in pred:\n",
    "        if det is not None and len(det):\n",
    "            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], frame.shape).round()\n",
    "            for *xyxy, conf, cls in det:\n",
    "                label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "                plot_one_box(xyxy, frame, label=label, color=(0, 255, 0), line_thickness=2)\n",
    "                if model.names[int(cls)] == 'accident':\n",
    "                    accident_detected = True\n",
    "                    break\n",
    "\n",
    "    if accident_detected:\n",
    "        accident_frames += 1\n",
    "        if accident_frames >= accident_frames_threshold and not alert_displayed:\n",
    "            cv2.putText(frame, 'Accident Detected!', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            alert_displayed = True\n",
    "    else:\n",
    "        accident_frames = 0\n",
    "        alert_displayed = False\n",
    "\n",
    "    cv2.imshow('ACCIDENT DETECTOR', frame)\n",
    "    if cv2.waitKey(50) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model will detect the presence of an accident scenario in the video but it will not send the alert here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 -Code to Detect Accident and send the accident message along with the accident image and time to a Telegram Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 157 layers, 7023610 parameters, 0 gradients, 15.8 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n",
      "Message sent successfully!\n"
     ]
    }
   ],
   "source": [
    "weights = 'o&a.pt'  # Path to model weights\n",
    "model = attempt_load(weights)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  \n",
    "stride = int(model.stride.max()) \n",
    "imgsz = 640  # Input image size\n",
    "\n",
    "# Load the video capture\n",
    "cap = cv2.VideoCapture(\"accidentvideo.mp4\")\n",
    "# create function to send alert to telegram\n",
    "bot_token = 'Github will not permit the use of private token. So we are not able to provide that'\n",
    "channel_id = \"@PROJECT_520\"\n",
    "def send_telegram_message_to_channel(bot_token, chat_id, message, photo=None):\n",
    "    url = f\"https://api.telegram.org/bot{bot_token}/sendPhoto\" if photo else f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
    "    params = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"caption\": message\n",
    "    }\n",
    "    files = {'photo': ('accident.jpg', photo)} if photo else None\n",
    "    response = requests.post(url, params=params, files=files)\n",
    "    if response.status_code == 200:\n",
    "        print(\"Message sent successfully!\")\n",
    "    else:\n",
    "        print(f\"Failed to send message. Status code: {response.status_code}\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    img0 = letterbox(frame, new_shape=imgsz)[0]\n",
    "    img = img0[:, :, ::-1].transpose(2, 0, 1) \n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float()  \n",
    "    img /= 255.0 \n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, 0.4, 0.5, classes=None, agnostic=False)\n",
    "\n",
    "    # Display the results and send message if accident detected\n",
    "    for det in pred:\n",
    "        if det is not None and len(det):\n",
    "            det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], frame.shape).round()\n",
    "            for *xyxy, conf, cls in det:\n",
    "                label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "                if model.names[int(cls)] == 'accident' and conf > 0.80:\n",
    "                    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    message = f\"Accident Detected at {current_time}!\"\n",
    "\n",
    "                    # Capture the accident frame\n",
    "                    accident_frame = frame.copy()\n",
    "\n",
    "                    # Save the accident frame as an image\n",
    "                    cv2.imwrite(\"accident.jpg\", accident_frame)\n",
    "\n",
    "                    # Send the message and photo\n",
    "                    send_telegram_message_to_channel(bot_token, channel_id, message, photo=open(\"accident.jpg\", \"rb\"))\n",
    "\n",
    "                   \n",
    "\n",
    "                plot_one_box(xyxy, frame, label=label, color=(0, 255, 0), line_thickness=2)\n",
    "\n",
    "    cv2.imshow('ACCIDENT DETECTOR', frame)\n",
    "    if cv2.waitKey(50) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model will detect the presence of an accident in the video and if the accident detected in the frame have a probability of more than 80% then it will capture the image of the accident occuring frame and save it to accident.jpg and then it will utilize the telegram api to send the \"Accident Detected\" message along with the captured image and the time of the accident occured to the corresponding channel in the telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
